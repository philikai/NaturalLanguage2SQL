{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Strategies for Fine-Tuning CodeLlama using Quantized Low-Rank Adaptation (QLoRA) on Amazon SageMaker\n",
    "\n",
    "This notebook provides insights into the application of QLoRA for fine-tuning the CodeLlama model within the Amazon SageMaker environment.\n",
    "\n",
    "![Image generated by the author through Leonardo.ai]\n",
    "\n",
    "The realm of Natural Language to SQL (NL2SQL) has been an area of active exploration in recent years, especially as the integration of large-scale language models into business applications continues to evolve at a rapid pace.\n",
    "\n",
    "There has been a significant shift in the community's approach, moving from smaller, specialized models for NL2SQL tasks to larger, commercially available Large Language Models (LLMs) such as GPT-4, Anthropic Claude, or Amazon Titan. These models demonstrate remarkable zero-shot capabilities on standard benchmarks, like [SPIDER](https://yale-lily.github.io/spider), but their performance can be greatly enhanced through few-shot prompting. This shift offers promising avenues for ML practitioners to leverage natural language processing for data analytics, though challenges persist beyond benchmark scenarios.\n",
    "\n",
    "## Small Language Models\n",
    "\n",
    "Let's explore why you might opt to fine-tune a smaller model, such as [CodeLlama](https://huggingface.co/codellama/CodeLlama-7b-hf) or [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1), specifically for NL2SQL tasks:\n",
    "\n",
    "- **Data Privacy and Control**: Fine-tuning an open-source model gives you complete control over your data, whether running on the cloud or on-premise.\n",
    "- **Beneficial Overfitting**: Fine-tuning on your data means the model is familiar with your tables and databases, a positive aspect in this context.\n",
    "- **LoRA Adapters**: [LoRA](https://huggingface.co/blog/trl-peft) adapters can be dynamically loaded and unloaded, allowing for easy adaptation to changes in database schemas.\n",
    "- **Finding the Right Information**: Fine-tuning assists in accurately identifying relevant information, especially in systems with non-descriptive table and column names.\n",
    "\n",
    "In this notebook, we'll delve into the [Llama2](https://arxiv.org/abs/2307.09288) model family, specifically [CodeLlama](https://huggingface.co/codellama), known for its superior performance in NL2SQL tasks. Our goal is to guide you through the process of fine-tuning CodeLlama using QLoRA on Amazon SageMaker, including aspects like chunk length, table schema format, and synthetic data generation.\n",
    "\n",
    "**Notebook Overview:**\n",
    "\n",
    "- A guide on preparing data and fine-tuning CodeLlama with QLoRA on Amazon SageMaker.\n",
    "- Analyzing the impact of chunk length on the model's performance.\n",
    "- Deploying the model on Amazon SageMaker for production use.\n",
    "- Ideas for further improving the methodology with your company's data.\n",
    "\n",
    "Our focus extends beyond achieving high benchmark scores; we aim to adapt the model effectively for practical business applications.\n",
    "\n",
    "## Why CodeLlama?\n",
    "\n",
    "CodeLlama stands out due to its specialization, having been trained on a vast corpus of 500 billion coding-related tokens. This extensive training enables it to handle complex coding tasks and autocomplete scenarios with a high degree of accuracy.\n",
    "\n",
    "For a comprehensive understanding of CodeLlama's training methodology, refer to the [original paper](https://arxiv.org/abs/2307.09288) by Rozière et al., 2023, or the diagram below.\n",
    "\n",
    "![Training Process](assets/training-process-codellama.png)\n",
    "\n",
    "## QLoRA and PEFT\n",
    "\n",
    "Our approach with CodeLlama on Amazon SageMaker leverages [QLoRA](https://arxiv.org/abs/2305.14314), notable for its efficient quantization of pre-trained language models to 4 bits. This technique, a form of Parameter-Efficient Fine-Tuning (PEFT), allows us to fine-tune models with minimal resource usage. PEFT methods like those in the Huggingface [PEFT library](https://github.com/huggingface/peft) are invaluable for adapting large models efficiently, supported by tools like [Accelerate](https://huggingface.co/docs/accelerate/index) and [DeepSpeed](https://github.com/microsoft/DeepSpeed).\n",
    "\n",
    "Special thanks to Phil Schmid from Huggingface for his [insightful blog](https://www.philschmid.de/sagemaker-llama2-qlora) on fine-tuning LLaMA 2 models, which has significantly influenced this notebook.\n",
    "\n",
    "## 1. Setting Up the Development Environment\n",
    "\n",
    "To embark on this fine-tuning journey with Amazon SageMaker, ensure that your laptop is properly configured to interact with Amazon SageMaker.\n",
    "\n",
    "Initial setup involves configuring access to:\n",
    "\n",
    "1. Hugging Face — to access the CodeLlama model.\n",
    "2. Weights & Biases — for tracking training progress with the Wanda library.\n",
    "3. Amazon SageMaker — for conducting the training.\n",
    "\n",
    "Furthermore, we need to make sure that the correct libraries are installed. CodeLlama, for instance, is available from `transformers version 4.33.0.` Use the following commands to ensure that you have the needed packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/philikai/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philikai/opt/anaconda3/envs/FineTuningSageMaker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf-transfer in /Users/philikai/opt/anaconda3/envs/FineTuningSageMaker/lib/python3.10/site-packages (0.1.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "required_packages = [\n",
    "    \"sagemaker>=2.192.0\",\n",
    "    \"huggingface_hub\",\n",
    "    \"hf-transfer\",\n",
    "    \"transformers==4.33.0\",\n",
    "    \"datasets\",\n",
    "    \"wandb\",\n",
    "    \"matplotlib\",\n",
    "]\n",
    "\n",
    "# Check if the required packages are installed\n",
    "not_installed = []\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.split(\"=\")[0].split(\">\")[0])\n",
    "    except ImportError:\n",
    "        not_installed.append(package)\n",
    "\n",
    "# Install the missing packages\n",
    "if not_installed:\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", *not_installed, \"--upgrade\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access CodeLlama assets, log into your [Hugging Face account](https://huggingface.co/welcome). Create a `.env` file that holds your Huggingface Hub token. If you work locally, you can take advantage of it and specify your [SageMaker Execution](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-amazonsagemakerfullaccess-policy) role there as well. We will use [dotenv](https://pypi.org/project/python-dotenv/) to read the sensitive information from there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/philikai/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['huggingface-cli', 'login', '--token', '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'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Hugging Face token from the environment variable\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Check if the token is available\n",
    "if huggingface_token is None:\n",
    "    raise ValueError(\"Hugging Face token not found. Please check your .env file.\")\n",
    "\n",
    "# Login using the Hugging Face CLI with the token\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", huggingface_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tracking purposes, we are going to connect the SageMaker training container to [Weights and Biases](https://wandb.ai/site), where we can comfortably check the progress made, even when on the go.\n",
    "\n",
    "Next, log into your [wandb account](https://wandb.ai/home) from the output cell output. `wandb.sagemaker_auth(path=”scripts”)` saves the login data to a `scripts` folder that we are going to pass on to the training container. wandb will fetch the login details from there automatically. However, we still need to make sure to install wandb in the training container — which will be done via the `requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilikai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.sagemaker_auth(path=\"scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment, you need access to an AWS Identity and Access Management(IAM) role with the required permissions for Sagemaker. You can find more information about it [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). Next, we are going to set up all our connections to the Amazon SageMaker service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/philikai/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/philikai/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::510646607739:role/service-role/AmazonSageMaker-ExecutionRole-20220809T104176\n",
      "sagemaker bucket: sagemaker-us-east-1-510646607739\n",
      "sagemaker session region: us-east-1\n",
      "SageMaker Version: 2.197.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "local_mode = True\n",
    "\n",
    "if not local_mode:\n",
    "    if sagemaker_session_bucket is None and sess is not None:\n",
    "        # set to default bucket if a bucket name is not given\n",
    "        sagemaker_session_bucket = sess.default_bucket()\n",
    "    try:\n",
    "        role = sagemaker.get_execution_role()\n",
    "    except ValueError:\n",
    "        iam = boto3.client(\"iam\")\n",
    "        role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "else:\n",
    "    # Load .env file\n",
    "    load_dotenv()\n",
    "    # Get the SageMaker execution role\n",
    "    role = os.getenv(\"SAGEMAKER_EXECUTION_ROLE\")\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"SageMaker Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\\. Load and prepare the dataset\n",
    "\n",
    "Our experiment will be based on the [Spider Dataset](https://arxiv.org/abs/1809.08887), which is an open-source dataset for natural language to SQL, licensed under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/legalcode) license. In a preprocessing step, I enriched the dataset that is available on the HuggingFace hub with more information on the table schemas, foreign and primary keys for fine-tuning Llama models. SQL-PaLM inspired this enrichment step and the dataset is available on the [Huggingface dataset hub](https://huggingface.co/datasets/philikai/Spider-SQL-LLAMA2_train).\n",
    "\n",
    "As the final dataset would most probably be used as a tool in a chat setting, our final dataset should reflect an instruction tuning approach, which incorporates instruction, context, and answer, as shown by an example below.\n",
    "\n",
    "In the answer, we want CodeLlama to include <SQL></SQL> tags for the SQL query for easier parsing of the output.\n",
    "\n",
    "After formatting, a training example has the following parts\n",
    "\n",
    "**Instruction**  \n",
    "An instruction helps the model to pick up the intent. This instruction has not been tuned. Feel free to improve it!\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "### Instruction\n",
    "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables.\n",
    "The foreign and primary keys will be supplied. Write query in between <SQL></SQL>.\n",
    "Answer the following question with the context below:\n",
    "```\n",
    "\n",
    "**Context**\n",
    "\n",
    "The aim is to include information about the table structure via the “context” of our query. This information can be extracted from the table itself. Below is a shortened example of a relatively simple database.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "### Context\n",
    "[Schema (values) (types)]: | driving_school |  Addresses : address_id (text) , line_1_number_building (number) , city (text) , zip_postcode (text) , state_province_county (text) , country (text) | Staff : staff_id (text) , staff_address_id (number) , nickname (text) , first_name (text) , middle_name (text) , last_name (text) , date_of_birth (text) , date_joined_staff (number) , date_left_staff (number) | Vehicles : vehicle_id (text) , vehicle_details (number) | Customers : customer_id (text) , customer_address_id (number) , customer_status_code (text) , date_became_customer (text) , date_of_birth (text) , first_name (text) , last_name (text) , amount_outstanding (number) , email_address (number) , phone_number (text) , cell_mobile_phone_number (text) | Customer_Payments : customer_id (text) , datetime_payment (number) , payment_method_code (text) , amount_payment (text) | Lessons : lesson_id (text) , customer_id (number) , lesson_status_code (text) , staff_id (text) , vehicle_id (text) , lesson_date (text) , lesson_time (text) , price (number); | [Foreign Keys]: staff : staff_address_id = addresses : address_id | customers : customer_address_id = addresses : address_id | customer_payments : customer_id = customers : customer_id | lessons : customer_id = customers : customer_id | lessons : staff_id = staff : staff_id | lessons : vehicle_id = vehicles : vehicle_id | [Primary Keys]: addresses : address_id, staff : staff_id, vehicles : vehicle_id, customers : customer_id, customer_payments : customer_id, lessons : lesson_id\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Task**\n",
    "\n",
    "Experimentation with API-based and non-fine-tuned model have shown that controlling for the output — to only include the SQL query — can be a challenging task. We want our model to follow precisely our formatting rules, even when running thousands of queries. For responses in JSON format, update the instructions.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "### Answer\n",
    "<SQL> SELECT T1.state_province_county FROM Addresses AS T1 JOIN Staff AS T2 ON T1.address_id = T2.staff_address_id GROUP BY T1.state_province_county HAVING count(*) BETWEEN 2 AND 4; </SQL>\"\"\"\n",
    "```\n",
    "\n",
    "To load the `philikai/Spider-SQL-LLAMA2_train` dataset, we use the `load_dataset()` method from the 🤗 Datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 808/808 [00:00<00:00, 3.27MB/s]\n",
      "Downloading data: 100%|██████████| 546k/546k [00:00<00:00, 2.66MB/s]\n",
      "Downloading data: 100%|██████████| 73.5k/73.5k [00:00<00:00, 461kB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  5.33it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 949.37it/s]\n",
      "Generating train split: 100%|██████████| 8659/8659 [00:00<00:00, 416921.84 examples/s]\n",
      "Generating validation split: 100%|██████████| 1034/1034 [00:00<00:00, 309028.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philikai/Spider-SQL-LLAMA2_train\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the dataset, we find the following features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': Value(dtype='string', id=None),\n",
       " 'query': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'schema': Value(dtype='string', id=None),\n",
       " 'primary_keys': Value(dtype='string', id=None),\n",
       " 'foreign_keys': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruction tune our model via the prompt parts outlined above, we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the instruction prompt to maximize the model performance further\n",
    "def format_spider(sample):\n",
    "    instruction_prompt = f\"\"\"Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
    "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
    "    Answer the following question with the context below: \\n{sample['question']}\"\"\"\n",
    "    instruction = f\"### Instruction\\n{instruction_prompt} \"\n",
    "    context = f\"### Context\\n{sample['schema']} | {sample['foreign_keys']} | {sample['primary_keys']}\"\n",
    "    response = f\"### Answer\\n<SQL> {sample['query']} </SQL>\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "Which student's age is older than 18 and is majoring in 600? List each student's first and last name. \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | restaurant_1 |  Student : stuid (text) , lname (number) , fname (text) , age (text) , sex (number) , major (text) , advisor (number) , city_code (number) | Restaurant : resid (text) , resname (number) , address (text) , rating (text) | Type_Of_Restaurant : resid (text) , restypeid (number) | Restaurant_Type : restypeid (text) , restypename (number) , restypedescription (text) | Visits_Restaurant : stuid (text) , resid (number) , time (text) , spent (text); | [Foreign Keys]: type_of_restaurant : restypeid = restaurant_type : restypeid | type_of_restaurant : resid = restaurant : resid | visits_restaurant : resid = restaurant : resid | visits_restaurant : stuid = student : stuid | [Primary Keys]: student : stuid, restaurant : resid, type_of_restaurant : restypeid\n",
      "\n",
      "### Answer\n",
      "<SQL> SELECT Fname , Lname FROM Student WHERE Age  >  18 AND Major = 600; </SQL>\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_spider(dataset[\"train\"][randrange(len(dataset[\"train\"]))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the dataset\n",
    "\n",
    "Although an integral part of any LLM application, the role of the tokenizer is often overlooked. However, adding or forgetting to add a `eos_token` to your datasets samples can make the difference between a successful or failed fine-tuning job.  \n",
    "You can download the right tokenizer from the Huggingface Hub. The pad_token gets set to the EOS token to ensure that the model will pick it up during the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philikai/opt/anaconda3/envs/FineTuningSageMaker/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"codellama/CodeLlama-7b-hf\"  # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['db_id', 'query', 'question', 'schema', 'primary_keys', 'foreign_keys'],\n",
      "    num_rows: 8659\n",
      "})\n",
      "Dataset({\n",
      "    features: ['db_id', 'query', 'question', 'schema', 'primary_keys', 'foreign_keys'],\n",
      "    num_rows: 1034\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# assign just the train dataset for testing purposes\n",
    "dataset_train = dataset[\"train\"]\n",
    "dataset_validation = dataset[\"validation\"]\n",
    "\n",
    "# remove 'text' column if it exists in the dataset_train\n",
    "if \"text\" in dataset_train.column_names:\n",
    "    dataset_train = dataset_train.remove_columns(\"text\")\n",
    "print(dataset_train)\n",
    "\n",
    "# remove 'text' column if it exists in the dataset_validation\n",
    "if \"text\" in dataset_validation.column_names:\n",
    "    dataset_validation = dataset_validation.remove_columns(\"text\")\n",
    "print(dataset_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When templating the dataset, adding an eos_token to the end of every sample, ensures that the model will stop generating after it has finished the SQL code. Furthermore, we fine-tune the model to wrap all the output code in <SQL> </SQL> tags, for us to easily parse the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8659/8659 [00:00<00:00, 12497.85 examples/s]\n",
      "Map: 100%|██████████| 1034/1034 [00:00<00:00, 13188.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "How much was the budget of \" Finding Nemo \" \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | imdb |  actor : aid (text) , gender (number) , name (text) , nationality (text) , birth_city (text) , birth_year (text) | copyright : id (text) , msid (number) , cid (text) | cast : id (text) , msid (number) , aid (text) , role (text) | genre : gid (text) , genre (number) | classification : id (text) , msid (number) , gid (text) | company : id (text) , name (number) , country_code (text) | director : did (text) , gender (number) , name (text) , nationality (text) , birth_city (text) , birth_year (text) | producer : pid (text) , gender (number) , name (text) , nationality (text) , birth_city (text) , birth_year (text) | directed_by : id (text) , msid (number) , did (text) | keyword : id (text) , keyword (number) | made_by : id (text) , msid (number) , pid (text) | movie : mid (text) , title (number) , release_year (text) , title_aka (text) , budget (text) | tags : id (text) , msid (number) , kid (text) | tv_series : sid (text) , title (number) , release_year (text) , num_of_seasons (text) , num_of_episodes (text) , title_aka (text) , budget (number) | writer : wid (text) , gender (number) , name (text) , nationality (text) , num_of_episodes (text) , birth_city (text) , birth_year (number) | written_by : id (text) , msid (number) , wid (text); | [Foreign Keys]: cast : msid = copyright : msid | cast : aid = actor : aid | classification : msid = copyright : msid | classification : gid = genre : gid | directed_by : did = director : did | directed_by : msid = copyright : msid | made_by : pid = producer : pid | made_by : msid = copyright : msid | tags : msid = copyright : msid | written_by : wid = writer : wid | written_by : msid = copyright : msid | [Primary Keys]: actor : aid, copyright : id, cast : id, genre : gid, classification : id, company : id, director : did, producer : pid, directed_by : id, keyword : id, made_by : id, movie : mid, tags : id, tv_series : sid, writer : wid\n",
      "\n",
      "### Answer\n",
      "<SQL> SELECT budget FROM movie WHERE title  =  \"Finding Nemo\"; </SQL></s>\n",
      "**********************************************************************************************************************************************************************************************************************************************************\n",
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "Return the names of poker players sorted by their earnings descending. \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | poker_player |  poker_player : poker_player_id (text) , people_id (number) , final_table_made (number) , best_finish (number) , money_rank (number) , earnings (number) | people : people_id (text) , nationality (number) , name (number) , birth_date (number) , height (number); | [Foreign Keys]: poker_player : people_id = people : people_id | [Primary Keys]: poker_player : poker_player_id, people : people_id\n",
      "\n",
      "### Answer\n",
      "<SQL> SELECT T1.Name FROM people AS T1 JOIN poker_player AS T2 ON T1.People_ID  =  T2.People_ID ORDER BY T2.Earnings DESC </SQL></s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_spider(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset_train_format_ok = dataset_train.map(\n",
    "    template_dataset, remove_columns=list(dataset_train.features)\n",
    ")\n",
    "\n",
    "dataset_train_format_ok_val = dataset_validation.map(\n",
    "    template_dataset, remove_columns=list(dataset_validation.features)\n",
    ")\n",
    "# print random sample\n",
    "print(dataset_train_format_ok[randint(0, len(dataset_train_format_ok))][\"text\"])\n",
    "print(\"*\" * 250)\n",
    "print(dataset_train_format_ok_val[randint(0, len(dataset_train_format_ok_val))][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `</s>` has been added at the end of the example solution. CodeLlama will lean throughout the fine-tuning that it has to close every example with </SQL>, followed by a </s>. As soon as the </s> has been predicted, the generation will stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 835, 2799, 4080, 13, 29954, 5428, 385, 1881, 1139, 29892, 671, 21120, 5877, 304, 5706, 263, 4576, 2346, 491, 23906, 697, 470, 2999, 310, 278, 1494, 6131, 29889, 29871, 13, 1678, 450, 9117, 322, 7601, 6611, 674, 367, 19056, 29889, 14350, 2346, 297, 1546, 529, 4176, 2565, 4176, 15513, 29871, 13, 1678, 673, 278, 1494, 1139, 411, 278, 3030, 2400, 29901, 29871, 13, 5328, 1784, 15883, 310, 278, 5840, 1860, 526, 9642, 1135, 29871, 29945, 29953, 1577, 29871, 13, 13, 2277, 29937, 15228, 13, 29961, 12763, 313, 5975, 29897, 313, 8768, 4638, 29901, 891, 14311, 29918, 21895, 891, 29871, 14311, 584, 14311, 29918, 333, 313, 726, 29897, 1919, 1024, 313, 4537, 29897, 1919, 11265, 313, 726, 29897, 1919, 24034, 313, 726, 29897, 1919, 23562, 29918, 262, 29918, 29890, 453, 1080, 313, 4537, 29897, 1919, 954, 29918, 3451, 2376, 12712, 313, 4537, 29897, 891, 2343, 584, 2343, 29918, 333, 313, 726, 29897, 1919, 1024, 313, 4537, 29897, 1919, 6345, 29918, 3859, 313, 726, 29897, 1919, 5046, 313, 726, 29897, 891, 10643, 584, 14311, 29918, 333, 313, 726, 29897, 1919, 2343, 29918, 333, 313, 4537, 29897, 1919, 13201, 29918, 627, 292, 313, 726, 416, 891, 518, 27755, 4813, 952, 5387, 10643, 584, 2343, 29918, 333, 353, 2343, 584, 2343, 29918, 333, 891, 10643, 584, 14311, 29918, 333, 353, 14311, 584, 14311, 29918, 333, 891, 518, 26666, 4813, 952, 5387, 14311, 584, 14311, 29918, 333, 29892, 2343, 584, 2343, 29918, 333, 29892, 10643, 584, 14311, 29918, 333, 13, 13, 2277, 29937, 673, 13, 29966, 4176, 29958, 5097, 2302, 22798, 3895, 2343, 5754, 5046, 29871, 1405, 259, 29945, 29953, 1533, 4176, 29958, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets inspect what we get from the tokenizer\n",
    "tokenizer(dataset_train_format_ok[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After careful examination of the input ids, we can find the eos_token to be `2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/8659 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8659/8659 [00:03<00:00, 2879.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tok_dataset = dataset_train_format_ok.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset_train_format_ok.features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of chunk length on NL2SQL fine-tuning\n",
    "\n",
    "The next step would be to run a chunking and batching operation on our dataset. Many ML practitioners choose just to apply defaults to those operations. However, we want first to examine our dataset regarding the token length.\n",
    "\n",
    "Histogram of the input token lengths (instruction and solution) for training. The token length is strongly governed by the complexity of the database, leading to distinctive spikes.\n",
    "\n",
    "Observe how the token length has distinctive spikes stemming from the database schema description. As we supply the full database description, the model has to find the right tables and columns in the context.\n",
    "\n",
    "Next, we want to chunk our dataset and batch it. This is done via the following code snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of any entry: 2840\n",
      "Number of entries above 2048 tokens: 82\n"
     ]
    }
   ],
   "source": [
    "def get_max_length_and_count(dataset, max_token_length):\n",
    "    \"\"\"\n",
    "    Given a dataset, this function returns the maximum length of any of the entries and counts how many\n",
    "    of them have a length above the specified max_token_length. It also plots a histogram of the lengths.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (dict): Dictionary containing 'input_ids' as keys and lists as values.\n",
    "    - max_token_length (int): Specified max token length to compare with.\n",
    "\n",
    "    Returns:\n",
    "    - (int, int): Maximum length of any of the entries and count of entries having a length above max_token_length.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extracting all lengths\n",
    "    lengths = [len(entry) for entry in dataset[\"input_ids\"]]\n",
    "\n",
    "    # Getting the maximum length\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    # Counting how many are above the specified max_token_length\n",
    "    count_above_max_token_length = sum(\n",
    "        1 for length in lengths if length > max_token_length\n",
    "    )\n",
    "\n",
    "    # Plotting a histogram of the lengths\n",
    "    plt.hist(lengths, bins=100)\n",
    "    plt.xlabel(\"Token Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Token Lengths\")\n",
    "    plt.show()\n",
    "\n",
    "    return max_length, count_above_max_token_length\n",
    "\n",
    "\n",
    "max_length_value = 2048\n",
    "max_len, count_above = get_max_length_and_count(tok_dataset, max_length_value)\n",
    "print(f\"Maximum length of any entry: {max_len}\")\n",
    "print(f\"Number of entries above {max_length_value} tokens: {count_above}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the token length has distinctive spikes stemming from the database schema description. As we supply the full database description, the model has to find the right tables and columns in the context.\n",
    "\n",
    "Next, we want to chunk our dataset and batch it. This is done via the following code snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8659/8659 [00:04<00:00, 2161.14 examples/s]\n",
      "Map: 100%|██████████| 1034/1034 [00:00<00:00, 3835.68 examples/s]\n",
      "Map: 100%|██████████| 1034/1034 [00:00<00:00, 2738.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 2264\n",
      "Total number of samples: 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {\n",
    "        k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()\n",
    "    }\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {\n",
    "        k: concatenated_examples[k][batch_chunk_length:]\n",
    "        for k in concatenated_examples.keys()\n",
    "    }\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk training dataset\n",
    "lm_dataset = dataset_train_format_ok.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset_train_format_ok.features),\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "\n",
    "# tokenize and chunk validation dataset\n",
    "lm_dataset_validation = dataset_train_format_ok_val.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset_train_format_ok_val.features),\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n",
    "print(f\"Total number of samples: {len(lm_dataset_validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a scenario where you have a decoder-only model for natural language processing, such as GPT-style models, and you’re working with instruction and solution pairs where the instruction is significantly longer than the solution (e.g., 3000 tokens for instruction and 500 tokens for the solution) and a chunk length is smaller than the chunk length, consider the following aspects.\n",
    "\n",
    "- **Tokenization and Chunking**  \n",
    "  Instruction (e.g., 3000 tokens) and Solution (e.g., 500 tokens): Both the instruction and the solution are tokenized. Given an example chunk size of 2048 tokens, the instruction exceeds this limit significantly.  \n",
    "  The instruction is, therefore, chunked into segments. The first chunk will contain 2048 tokens from the instruction. The remaining 952 tokens become part of the next chunk.  \n",
    "  The solution, being only 500 tokens, can fit entirely in a chunk without needing to be split.\n",
    "- **Sequence Formation**  \n",
    "  In decoder-only models, the input is often formatted as a single sequence where the instruction and solution are concatenated, typically with a special token separating them (in our case, we train on `### Answer)`\n",
    "- **Dealing with Length Discrepancy**  \n",
    "  First Chunk: The first 2048 tokens of the instruction are processed. But since there’s no room left in this chunk for the solution, the solution is not included here.  \n",
    "  Second Chunk: The remaining 952 tokens of the instruction are placed in the next chunk, and here, the 500-token solution can also be included, as the total tokens (952 + 500) are within the 2048 token limit.\n",
    "- **Casual Language Modeling (CLM)**  \n",
    "  Decoder-Only Model: This type of model generates text one token at a time, predicting the next token based on the previous tokens. It does not use the bidirectional context like encoder models.  \n",
    "  Training: During training, the model learns to predict the next token in the sequence. For our dataset it learns to continue the text from the instruction to the solution.  \n",
    "  Attention Masking: The model uses attention mechanisms to weigh the importance of different tokens in the sequence when predicting the next token. In a chunk containing both instruction and solution, it learns the transition from the instructional context to the solution context.\n",
    "\n",
    "**Training Implications  \n",
    "**The main challenge is that the model may not always see the instruction and solution together in the same chunk, especially for very long instructions. This can impact its ability to learn the relationship between specific instructions and their solutions.  \n",
    "Partial Context: In cases where the instruction is cut, the model gets only a part of the instruction in one chunk and the rest with the solution in the next. This affects the learning process, as the model doesn’t always see the complete instruction with the corresponding solution.\n",
    "\n",
    "In many datasets, instruction lengths are often far below the chunk window. However, for complex databases, schema information can get very long and exceed the chunk length. The implication is that we are not training optimally on our most complex examples.\n",
    "\n",
    "To understand the impact of the chunk length in our dataset, we kicked off four training jobs with 256, 512, 1024, and 4096 tokens.\n",
    "\n",
    "Finally, upload your datasets to Amazon S3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to:\n",
      "Training dataset: s3://sagemaker-us-east-1-510646607739/processed/codellama/nl2sql/train\n",
      "Validation dataset: s3://sagemaker-us-east-1-510646607739/processed/codellama/nl2sql/validation\n"
     ]
    }
   ],
   "source": [
    "# Function to upload a directory to S3 bucket and verify upload\n",
    "def upload_directory_to_s3(bucket_name, directory_path, s3_prefix):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Walk through each file in the directory\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, directory_path)\n",
    "            s3_path = os.path.join(s3_prefix, relative_path)\n",
    "\n",
    "            # Upload file to S3\n",
    "            s3.upload_file(local_path, bucket_name, s3_path)\n",
    "            print(f\"Uploaded {local_path} to s3://{bucket_name}/{s3_path}\")\n",
    "\n",
    "            # Verify the upload\n",
    "            try:\n",
    "                # Get the metadata of the uploaded file\n",
    "                response = s3.head_object(Bucket=bucket_name, Key=s3_path)\n",
    "                s3_file_size = response[\"ContentLength\"]\n",
    "\n",
    "                # Compare the file size\n",
    "                local_file_size = os.path.getsize(local_path)\n",
    "                if local_file_size != s3_file_size:\n",
    "                    print(f\"Size mismatch for file: {local_path}\")\n",
    "                else:\n",
    "                    print(f\"Successfully verified the upload of {local_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error verifying file upload: {e}\")\n",
    "\n",
    "\n",
    "# Define your bucket name\n",
    "bucket_name = sess.default_bucket()\n",
    "\n",
    "# Upload the directories to S3\n",
    "upload_directory_to_s3(bucket_name, \"./tmp/train\", \"processed/codellama/nl2sql/train\")\n",
    "upload_directory_to_s3(\n",
    "    bucket_name, \"./tmp/validation\", \"processed/codellama/nl2sql/validation\"\n",
    ")\n",
    "\n",
    "# Define the S3 paths\n",
    "training_input_path = f\"s3://{bucket_name}/processed/codellama/nl2sql/train\"\n",
    "validation_input_path = f\"s3://{bucket_name}/processed/codellama/nl2sql/validation\"\n",
    "\n",
    "print(\"Uploaded data to:\")\n",
    "print(f\"Training dataset: {training_input_path}\")\n",
    "print(f\"Validation dataset: {validation_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\\. Fine-Tuning CodeLlama with QLoRA on Amazon SageMaker\n",
    "\n",
    "In order to optimize CodeLlama using the QLoRA methodology, we’ve drawn inspiration from the invaluable insights shared in Phil Schmid’s blog post on [fine-tuning LLaMA models with QLoRA on SageMaker](https://www.philschmid.de/sagemaker-llama2-qlora).\n",
    "\n",
    "From the same source we can reuse the [run_clm.py](https://github.com/philschmid/sagemaker-huggingface-llama-2-samples/blob/master/training/scripts/run_clm.py), which implements QLoRA using PEFT to train a model. Post-training, this script integrates the LoRA weights into the model’s architecture when setting `merge_weights=True`. For models that exceed memory capacity, temporary offloading to disk is implemented.\n",
    "\n",
    "Please take a look at the code snippet that merges the adapter weights:\n",
    "\n",
    "```python\n",
    "   sagemaker_save_dir=\"/opt/ml/model/\"\n",
    "    if args.merge_weights:\n",
    "        # merge adapter weights with base model and save\n",
    "        # save int 4 model\n",
    "        trainer.model.save_pretrained(output_dir, safe_serialization=False)\n",
    "        # clear memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "        # load PEFT model in fp16\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            output_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        # Merge LoRA and base model and save\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(\n",
    "            sagemaker_save_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )\n",
    "    else:\n",
    "        trainer.model.save_pretrained(\n",
    "            sagemaker_save_dir, safe_serialization=True\n",
    "        )\n",
    "```\n",
    "\n",
    "To initiate a SageMaker training job, we utilize a [HuggingFace Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html). This HuggingFace estimator simplifies the Amazon SageMaker training and deployment process by managing the necessary infrastructure. SageMaker orchestrates the setup and management of required EC2 instances, supplies the appropriate HuggingFace container, uploads scripts, and downloads data from our S3 bucket to the container at `/opt/ml/input/data`, before commencing the training job.\n",
    "\n",
    "For even easier LLM fine-tuning, you can try A[mazon SageMaker Jumpstart](https://aws.amazon.com/sagemaker/jumpstart/?p=pm&c=sm&z=2), which allows you to fine-tune a large set of models with a click of a button. At the time of writing, CodeLlama was not available on [Amazon SageMaker Jumpstart](https://aws.amazon.com/sagemaker/jumpstart/?p=pm&c=sm&z=2).\n",
    "\n",
    "An important aspect to keep in mind is that the HuggingFace Estimator version available on SageMaker may not always be in sync with the latest release of the [Transformers](https://pypi.org/project/transformers/) library. To address this, ensure that your environment is running the desired version of the Transformers library by specifying it in a `requirements.txt` file, which allows you to upgrade to a specific version or even install the latest version directly from the GitHub repository. By leveraging this approach, you gain the flexibility to work with the current features and updates from the transformers library, ensuring that your SageMaker environment is equipped with the cutting-edge tools needed for your machine-learning projects.\n",
    "aker environment is equipped with the cutting-edge tools needed for your machine learning projects.\n",
    "\n",
    "### Harwarde requirements\n",
    "\n",
    "Please take a look at a few selected options on how to run this training job.\n",
    "\n",
    "| Model                                                              | Instance Type      | Batch Size | Context Length |\n",
    "| ------------------------------------------------------------------ | ------------------ | ---------- | -------------- |\n",
    "| [CodeLlama 7B](https://huggingface.co/codellama/CodeLlama-7b-hf)   | `(ml.)g5.4xlarge`  | `3`        | `2048`         |\n",
    "| [CodeLlama 7B](https://huggingface.co/codellama/CodeLlama-7b-hf)   | `(ml.)g5.4xlarge`  | `9`        | `256`          |\n",
    "| [CodeLlama 7B](https://huggingface.co/codellama/CodeLlama-7b-hf)   | `(ml.)g5.4xlarge`  | `6`        | `512`          |\n",
    "| [CodeLlama 7B](https://huggingface.co/codellama/CodeLlama-7b-hf)   | `(ml.)g5.4xlarge`  | `4`        | `1024`         |\n",
    "| [CodeLlama 13B](https://huggingface.co/codellama/CodeLlama-13b-hf) | `(ml.)g5.4xlarge`  | `2`        | `2048`         |\n",
    "| [CodeLlama 13B](https://huggingface.co/codellama/CodeLlama-13b-hf) | `(ml.)g5.12xlarge` | `4`        | `2048`         |\n",
    "| [CodeLlama 13B](https://huggingface.co/codellama/CodeLlama-13b-hf) | `(ml.)g5.12xlarge` | `3`        | `3576`         |\n",
    "\n",
    "You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use the `merge_weights` parameter since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them `merge_adapter_weights.py` after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,  # pre-trained model\n",
    "    \"dataset_path\": \"/opt/ml/input/data/training\",  # path where sagemaker will save training dataset\n",
    "    \"epochs\": 5,  # number of training epochs\n",
    "    \"per_device_train_batch_size\": 4,  # batch size for training\n",
    "    \"lr\": 2e-4,  # learning rate used during training\n",
    "    \"hf_token\": HfFolder.get_token(),  # huggingface token to access llama 2\n",
    "    \"merge_weights\": True,  # wether to merge LoRA into the model (needs more memory)\n",
    "    \"report_to\": \"wandb\",  # ✍️\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"run_clm.py\",  # train script\n",
    "    source_dir=\"scripts\",  # directory which includes all the files needed for training\n",
    "    instance_type=\"ml.g5.12xlarge\",  # instances type used for the training job\n",
    "    instance_count=1,  # the number of instances used for training\n",
    "    base_job_name=job_name,  # the name of the training job\n",
    "    role=role,  # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=300,  # the size of the EBS volume in GB\n",
    "    transformers_version=\"4.28\",  # the transformers version used in the training job\n",
    "    pytorch_version=\"2.0\",  # the pytorch_version version used in the training job\n",
    "    py_version=\"py310\",  # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\"\n",
    "    },  # set env variable to cache models in /tmp\n",
    "    keepAlivePeriod=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training job with the `.fit()` method. The training container receives the training and validation dataset directly from s3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"training\": training_input_path, \"validation\": validation_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job will then start to run on AWS. Fine-tuning the 7B parameter model for 5 epochs on a ml.g5.4xlarge instance and a chunk length of 2048 took 17 hours in the us-east-1 region. Which equates to a cost of roughly 35 USD.\n",
    "\n",
    "As you might lose the kernel while the training job is executing on AWS, you can always attach back to a training job as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reattach to a SageMaker Estimator in case your notebook died during the training job\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "TrainingJobName = \"Your Training Job Name\"\n",
    "\n",
    "huggingface_estimator = HuggingFace.attach(TrainingJobName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case your can't remember the training job name, you can look it up via the management console or via the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface-qlora-2023-12-07-11-10-51-2023-12-07-10-10-53-727\n",
      "huggingface-qlora-13B-200k-training-202-2023-12-03-00-44-18-598\n",
      "huggingface-qlora-13B-200k-training-202-2023-12-02-22-14-58-279\n",
      "huggingface-qlora-13B-200k-training-202-2023-12-02-20-18-57-163\n",
      "pipelines-x6xndxsr6p6j-Untitledip-Untitled-zG7LcLJc45\n",
      "pipelines-t6zb0i394tzu-Untitledip-Untitled-uNESI7YyJ8\n",
      "pipelines-5i1xwx0ouu31-Untitledip-Untitled-cFQMYTrHWE\n",
      "pipelines-vq6ih9onq8im-Untitledip-Untitled-md7X6DU6NU\n",
      "pipelines-d3cxqhep6u7t-Untitledip-Untitled-pUo5xaT3oE\n",
      "huggingface-qlora-Codellama-7B-4096-chu-2023-11-24-21-52-25-616\n"
     ]
    }
   ],
   "source": [
    "# Create a SageMaker client\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "# List all training jobs\n",
    "# Get the latest training job\n",
    "response = sagemaker_client.list_training_jobs(\n",
    "    SortBy=\"CreationTime\", SortOrder=\"Descending\"\n",
    ")\n",
    "\n",
    "# Print the training job names\n",
    "for job in response[\"TrainingJobSummaries\"]:\n",
    "    print(job[\"TrainingJobName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to perform local testing, further analysis, or deployment of the model outside of SageMaker, you can of course download it. Please see the snippet below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data,  # S3 URI where the trained model is located\n",
    "    local_path=\"./codellama/\",  # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess,  # SageMaker session used for training the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, the s3 model uri will be sufficient for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model S3 URI: s3://sagemaker-us-east-1-510646607739/huggingface-qlora-2023-10-13-12-50-45-2023-10-13-12-50-49-361/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the model's S3 URI\n",
    "s3_model_uri = huggingface_estimator.model_data\n",
    "print(\"Model S3 URI:\", s3_model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\\. Deployment to an Amazon SageMaker Real-Time Endpoint\n",
    "\n",
    "Now that we have laid the foundations on how to fine-tune Code Llama on your own dataset, we need to deploy the model.\n",
    "\n",
    "With SageMaker, this is relatively simple, as it automatically saves a zipped version of your model to s3 when you successfully complete a training job.\n",
    "\n",
    "It is important that the archive directly contains all files and not a folder with the files — Amazon SageMaker takes care of that for you. If you train outside of SageMaker, your file should look like this:\n",
    "\n",
    "```\n",
    "\\`\\`\\`\n",
    "model.tar.gz/\n",
    "|- config.json\n",
    "|- model-00001-of-00005.safetensors\n",
    "|- tokenizer.json\n",
    "|- …\n",
    "\\`\\`\\`\n",
    "```\n",
    "\n",
    "You can use the [pigz-python](https://pypi.org/project/pigz-python/) package to parallelize the archiving.\n",
    "\n",
    "## Hugging Face TGI container in Amazon SageMaker\n",
    "\n",
    "There are many options you can choose from to run inference on your model, e.g., when creating benchmarks, one could simply extend the `run_clm.py` script to include a pass on the benchmark dataset at the end.\n",
    "\n",
    "Another way is to spin up a SageMaker endpoint with the Text Generation Inference container from Huggingface. First, retrieve the container URI that points to the desired Docker image. Amazon SageMaker’s `get_huggingface_llm_image_uri` method, being part of the Amazon SageMaker SDK, facilitates this. It allows for the acquisition of the Hugging Face LLM DLC’s URI, tailored to your specific requirements, including backend, session, region, and version. To explore available versions, refer to the [list of available images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/philikai/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py39\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.0.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\"huggingface\", version=\"1.0.3\")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the Fine-Tuned Code Llama on Amazon SageMaker\n",
    "\n",
    "To deploy the model such as CodeLlama on Amazon SageMaker, we create a HuggingFaceModel class. This class forms the basis of our endpoint configuration, encompassing parameters like `hf_model_id`, `instance_type`, and others. We opt for the `ml.g5.12xlarge` instance type, boasting 4 NVIDIA A10G GPUs and a significant 96GB of GPU memory.\n",
    "You can review the instance details on the offical [aws ec2 website](https://aws.amazon.com/de/ec2/instance-types/g5/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/philikai/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/philikai/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "### Deploying the Fine-Tuned Code Llama on Amazon SageMaker\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 500\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"/opt/ml/model\",  # path to where sagemaker stores the model\n",
    "    \"SM_NUM_GPUS\": json.dumps(number_of_gpu),  # Number of GPU used per replica\n",
    "    \"MAX_INPUT_LENGTH\": json.dumps(3072),  # Max length of input text\n",
    "    \"MAX_TOTAL_TOKENS\": json.dumps(\n",
    "        4096\n",
    "    ),  # Max length of the generation (including input text)\n",
    "    # 'HF_MODEL_QUANTIZE': \"bitsandbytes\",# Comment in to quantize\n",
    "}\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role, image_uri=llm_image, model_data=s3_model_uri, env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying the `HuggingFaceModel` on SageMaker is straightforward using the deploy method. We use the `ml.g5.12xlarge` instance for deployment, where the Text Generation Interface (TGI) will automatically distribute and shard the model across all available GPUs. Furthermore, we set the heatlh check timeout a bit higher, to give the container time to download large models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2023-12-07-10-14-37-548\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2023-12-07-10-14-38-692\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2023-12-07-10-14-38-692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "    container_startup_health_check_timeout=health_check_timeout,  # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a benchmark on Spider validation dataset\n",
    "\n",
    "We load the Spider SQL dataset for validation purposes and prepare to run a benchmark test. Prepare the dataset for the model in the same way we fine-tuned it. However, we exclude the answer to the question — it is our models job to predict this one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philikai/Spider-SQL-LLAMA2_train\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': Value(dtype='string', id=None),\n",
       " 'output': Value(dtype='string', id=None),\n",
       " 'db_id': Value(dtype='string', id=None),\n",
       " 'query': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'schema': Value(dtype='string', id=None),\n",
       " 'primary_keys': Value(dtype='string', id=None),\n",
       " 'foreign_keys': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data\n",
    "\n",
    "Here, we format the validation data from the Spider dataset to suit our requirements. This involves constructing a prompt that guides the model in generating SQL queries based on provided questions and context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the instruction prompt to maximize the model performance further\n",
    "def format_spider_validation(sample):\n",
    "    instruction_prompt = f\"\"\"Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
    "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
    "    Answer the following question with the context below: \\n{sample['question']}\"\"\"\n",
    "    instruction = f\"### Instruction\\n{instruction_prompt} \"\n",
    "    context = f\"### Context\\n{sample['schema']} | {sample['foreign_keys']} | {sample['primary_keys']}\"\n",
    "    response = f\"### Answer\\n\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running a Single Example\n",
    "\n",
    "We test a single example from the validation set to ensure the formatting and model interaction are working as expected. This helps in fine-tuning the prompt and hyperparameters for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example number: 855\n",
      "Picked this example: \n",
      "\n",
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "What are the orchestras that do not have any performances? \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | orchestra |  conductor : conductor_id (text) , name (number) , age (text) , nationality (number) , year_of_work (text) | orchestra : orchestra_id (text) , orchestra (number) , conductor_id (text) , record_company (number) , year_of_founded (text) , major_record_format (number) | performance : performance_id (text) , orchestra_id (number) , type (text) , date (number) , official_ratings_(millions) (text) , weekly_rank (number) , share (number) | show : show_id (text) , performance_id (number) , if_first_show (text) , result (number) , attendance (text); | [Foreign Keys]: orchestra : conductor_id = conductor : conductor_id | performance : orchestra_id = orchestra : orchestra_id | show : performance_id = performance : performance_id | [Primary Keys]: conductor : conductor_id, orchestra : orchestra_id, performance : performance_id\n",
      "\n",
      "### Answer\n",
      "\n",
      "**********************************************************************************************************************************************************************************************\n",
      "SELECT Orchestra FROM orchestra WHERE Orchestra_ID NOT IN (SELECT Orchestra_ID FROM performance)\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "example_nr = randrange(len(dataset[\"validation\"]))\n",
    "print(f\"Example number: {example_nr}\")\n",
    "random_example = format_spider_validation(dataset[\"validation\"][example_nr])\n",
    "\n",
    "print(f\"Picked this example: \\n\\n{random_example}\")\n",
    "print(\"*\" * 190)\n",
    "print(dataset[\"validation\"][example_nr][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SQL> SELECT orchestra FROM orchestra WHERE orchestra_id NOT IN (SELECT orchestra_id FROM performance) </SQL>\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters for llm\n",
    "payload = {\n",
    "    \"inputs\": random_example,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.001,\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "        \"stop\": [\"</s>\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print(response[0][\"generated_text\"][:-len(\"<human>:\")])\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the actual benchmark on the validation dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to run the actual benchmark on the entire validation dataset. This process involves iterating over the dataset, sending each formatted sample to the model, and collecting the responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1034/1034 [16:25<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "answers = []\n",
    "\n",
    "dataset_val = dataset[\"validation\"]\n",
    "\n",
    "start = time()\n",
    "for idx, sample in tqdm(enumerate(dataset_val), total=len(dataset_val)):\n",
    "    formatted_sample = format_spider_validation(dataset_val[idx])\n",
    "    # hyperparameters for llm execution\n",
    "    payload = {\n",
    "        \"inputs\": formatted_sample,\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.95,\n",
    "            \"temperature\": 0.001,\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"repetition_penalty\": 1.03,\n",
    "            \"stop\": [\"</s>\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # send request to endpoint\n",
    "    response = llm.predict(payload)\n",
    "    answer = response[0][\"generated_text\"]\n",
    "    answers.append(answer)\n",
    "\n",
    "duration = time() - start\n",
    "avg_duration = duration / len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and Analyzing the Results\n",
    "\n",
    "After running the benchmark, we save the results for further analysis. This allows us to evaluate the model's performance in generating SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Specify the directory and filename\n",
    "dir_name = \"./results\"\n",
    "file_name = \"answers_codellama\"\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "# Write the data to the file\n",
    "with open(os.path.join(dir_name, file_name), \"wb\") as fp:\n",
    "    pickle.dump(answers, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the results to determine how many of the model's responses correctly follow the expected format. This gives us an insight into the model's understanding and ability to generate SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034 answers have <SQL> at the beginning and </SQL> at the end.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a count\n",
    "count = 0\n",
    "\n",
    "# Loop through the answers\n",
    "for answer in answers:\n",
    "    if answer.startswith(\"<SQL>\") and answer.endswith(\"</SQL>\"):\n",
    "        count += 1\n",
    "\n",
    "print(f\"{count} answers have <SQL> at the beginning and </SQL> at the end.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Finally, to manage resources effectively, we clean up by deleting the model and endpoint. This is an important step to avoid unnecessary charges and keep the environment tidy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-tgi-inference-2023-12-07-10-14-37-548\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-tgi-inference-2023-12-07-10-14-38-692\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-tgi-inference-2023-12-07-10-14-38-692\n"
     ]
    }
   ],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sagemaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
